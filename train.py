# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LSanQ6dDitaRXaibzZljuQJu7QI1UjbM
"""

import os
import time
import torch
from model import DeepChemLLMWrapper
from dataset import get_tox21_datasets

# Configuration
SAVE_DIR = "./results/checkpoints"
os.makedirs(SAVE_DIR, exist_ok=True)
BATCH_SIZE = 4
EPOCHS = 1  # Testing purposes
LR = 2e-5
SAVE_STEPS = 500

# Load Data & Model
train_ds, valid_ds, _ = get_tox21_datasets()
model = DeepChemLLMWrapper(r=32)

optimizer = torch.optim.AdamW(model.model.parameters(), lr=LR)
model.model.train()

print(f"Starting Training... Logs will be printed below.")

step_global = 0
for epoch in range(EPOCHS):
    t0 = time.time()
    for X, y, w, ids in train_ds.iterbatches(batch_size=BATCH_SIZE):
        try:
            inputs = model.tokenizer(
                X.tolist(), padding=True, truncation=True, max_length=256, return_tensors="pt"
            ).to(model.model.device)

            outputs = model.model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            step_global += 1

            if step_global % 50 == 0:
                print(f"Epoch {epoch+1} | Step {step_global} | Loss: {loss.item():.4f}")

            if step_global % SAVE_STEPS == 0:
                print(f"Saving Checkpoint at Step {step_global}...")
                model.model.save_pretrained(f"{SAVE_DIR}/checkpoint_{step_global}")

        except Exception as e:
            print(f"Skipping batch due to error: {e}")
            continue

print("Training Complete.")
model.model.save_pretrained(f"{SAVE_DIR}/final_model")